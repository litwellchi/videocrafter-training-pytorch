[rank: 0] Global seed set to 23
[rank: 1] Global seed set to 23
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v1.10.0. You can import it from `pytorch_lightning.utilities` instead.
  rank_zero_deprecation(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus='0,1,')` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices='0,1,')` instead.
  rank_zero_deprecation(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 23
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[rank: 1] Global seed set to 23
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./cv_webvid/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name              | Type                   | Params
-------------------------------------------------------------
0 | model             | DiffusionWrapper       | 1.1 B 
1 | first_stage_model | AutoencoderKL          | 83.7 M
2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
-------------------------------------------------------------
1.1 B     Trainable params
437 M     Non-trainable params
1.6 B     Total params
6,328.690 Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val/loss_simple', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val/loss_vlb', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val/loss_simple_ema', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val/loss_vlb_ema', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val/loss_ema', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1558: PossibleUserWarning: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('train/epoch/idx', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('train/epoch/time', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('train/epoch/time_avg', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/loss_simple', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/loss_vlb', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/epoch/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/epoch/idx', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/epoch/time', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/epoch/time_avg', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/aifs4su/mmcode/videogen/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('train/epoch/time_avg_min', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
Epoch 0, global step 25: 'val/loss_simple_ema' reached 0.52140 (best 0.52140), saving model to './cv_webvid/checkpoints/epoch=0000-step=000025.ckpt' as top 3
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Epoch 1, global step 50: 'val/loss_simple_ema' reached 0.24221 (best 0.24221), saving model to './cv_webvid/checkpoints/epoch=0001-step=000050.ckpt' as top 3
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
Epoch 2, global step 75: 'val/loss_simple_ema' reached 0.12391 (best 0.12391), saving model to './cv_webvid/checkpoints/epoch=0002-step=000075.ckpt' as top 3
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
WARNING:root:ShardedDDP detected that the trainable params changed, either because of eval/train mode or parameter freezing/unfreeze.
slurmstepd: error: *** STEP 23872.1 ON dgx-119 CANCELLED AT 2024-01-24T02:45:07 ***
slurmstepd: error: *** JOB 23872 ON dgx-119 CANCELLED AT 2024-01-24T02:45:07 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
